{
  "agent_id": "coder4",
  "task_id": "task_2",
  "files": [
    {
      "name": "requirements.txt",
      "purpose": "Python dependencies",
      "priority": "high"
    },
    {
      "name": "preprocessing.py",
      "purpose": "Image preprocessing utilities",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.NE_2508.15082v1_From_Basic_Affordances_to_Symbolic_Thought_A_Comp",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.NE_2508.15082v1_From-Basic-Affordances-to-Symbolic-Thought-A-Comp with content analysis. Detected project type: computer vision (confidence score: 9 matches).",
    "key_algorithms": [
      "Hebbian",
      "Machine",
      "Lisa",
      "Same",
      "Complex",
      "Transformer",
      "Biological",
      "Caption",
      "Theoretical",
      "Process"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "\n--- chunk_2.txt ---\nPDF: cs.NE_2508.15082v1_From-Basic-Affordances-to-Symbolic-Thought-A-Comp.pdf\nChunk: 2/2\n==================================================\n\n--- Page 38 ---\nappears. The affordance, itself, is never explicitly represented, so the \u201cconvex hull\u201d is restricted to the set of specific circumstances in which the affordance was previously experienced and interpolations between them. By contrast, a system that can represent the affordance \u201ccan walk on\u201d as an invariant (e.g., using neurons that respond specifically to that affordance, independent of everything else) and bind it to arbitrary arguments can walk on any walkable surface in any new environment, even statistically unlikely ones. Indeed, the greatest danger posed by such a representation is a propensity to overgeneralize: To a young enough child, every quadruped is a \u201cdoggie\u201d.  Representing Relations  The capacity to bind single-place predicates to their arguments is sufficient to account for how people and other animals perceive basic affordances, but it is not sufficient to explain how we reason about the relations between things in the world.  A relational representation is a multi-place predicate of the form r (x, y,\u2026z), that expresses a relation, r, between two (or more) arguments, x, y\u2026z. At first blush, it may appear that basic affordances take this form, where r is the affordance, x is the stimulus possessing the affordance, and y is the observer, as in the affordance can-walk-on (that-surface, me). However, in the case of basic affordances, the second argument is always either me or (more likely) some part of me such as my right hand, my eye, my left foot, etc. For this reason, basic affordances can be reduced to single-place predicates, such as can-walk-on (x), or can-grasp (x), where the argument, x, is bound dynamically to the object (e.g., this-surface or this-object), but the second argument is absorbed into the predicate itself (in the case of can-walk-on, the absorbed argument is me, whereas in the case of can-grasp, it might be my-right-hand). We argue that this is why even animals without the capacity for relational representations (including fruit flies, cats, and monkeys, all the way up to chimps and bonobos; see Penn, et al., 2008) can make their way in the world but generally cannot reason about the relations between objects exterior to themselves. However, to manipulate the world as people do, it is necessary to understand the relations between various objects in the world and be able to reason about those relations as entities in their own right. The capacity to represent and reason about relations as explicit, independent entities is evident in our ability to reason using schemas and analogies. It is trivial for most adults to understand the analogy between the structure of the solar system and the Rutherford model of the atom, but our ability to do so rests on our understanding that the orbit (x, y) relation is the same thing in the context of orbit (planets, sun) as in orbit (electrons, nucleus). Recall that armed with a capacity for recursion, a representational system with a finite number of basic elements can represent an unbounded number of specific structures. The effect of relational representations on our capacity for generalization scales accordingly. If being able to dynamically bind invariants to one another makes it possible to generalize to new situations containing those invariants (e.g., by making it possible to generalize from, say, a single exposure to a tasty red berry to the inference that all red things are tasty), being able to represent relational structures explodes the convex hull, in part by making it possible to represent properties of the universe that cannot be expressed at all with single-place predicates.  Mapping-based Generalization The ability to represent relations gives a representational system unbounded expressive power but being able to map one relational structure onto another makes it possible to use one structure as a representation of another. Analogical reasoning is an instance of using the source analog as a representation of the target. For example, Gick & Holyoak\u2019s (1980, 1983) subjects used a story of a general sending troops to capture a fortress in the center of a city from multiple directions (the source analog) to infer that a doctor could use radiation converging from multiple directions to destroy a tumor (the target \n\n--- Page 39 ---\nanalog). In this analogy, the subjects are representing the radiation problem in terms of the story about the general, where the general represents the doctor, the fortress represents the tumor, the town represents the healthy tissue, and the troops represent the radiation. Using the general story to reason about the radiation problem in this way is not unlike using an equation to solve a problem in physics. In both cases, the relevant parts of the target are mapped to the relevant parts of the source, and the structure of the source dictates the solution to the target. To date, these capacities have only been observed in humans (Penn et al., 2008). Reasoning about one system in terms of another, better understood, system effectively brings everything that is known about the better-known system to bear on the problem of understanding the less well-known system. This process is akin to expanding the smaller convex hull (the less well-known system) by merging it with the convex hull of the better-known system to form a larger effective convex hull. The power of this capacity to expand the scope of our reasoning is evidenced by the ubiquity of analogical thinking in human mental life (Holyoak & Thagard, 1995). Whenever we make an analogy between two structures, we induce a more general schema specifying what the original systems have in common and deemphasizing the details on which they differ (see Gentner, 1983; Gick & Holyoak, 1980, 1983; Holyoak & Thagard, 1995). When we discover an analogy between two seemingly very different things, it is as though we have discovered a greater, more general truth about the universe. Language  In this paper, we have focused exclusively on inference and generalization, up to and including relational reasoning, particularly in the context of vision. But it is worth noting that both multi-place relations and mapping play essential roles in language. Accordingly, it is tempting to speculate that the principles we have discussed here may be relevant to the origins of language (see also Deacon, 1997). All languages express multi-argument relations, permitting recursion, which has been observed in all human languages, with one possible exception (Everett, 1986). As a representational system, any language must be mappable onto representations of the ideas expressed by the language. This is not to say that these mappings are always explicit, as they are in the case of analogical reasoning, but they are at least capable of being made explicit. Asked about the correspondences between, say, an image and a verbal description of that image, most people can point out which parts of the text refer to which parts of the image. A sentence is a representation of whatever it is about, so there is necessarily a mapping between parts of the sentence and aspects of the thing being represented, even if some of these mappings are not obvious. It is therefore at least possible that the same mutations that gave rise to our capacity for multi-place relations and structure mapping also played a central role in the evolution of language.  The Relationship Between Biological and Artificial Intelligence    The apparent successes of transformer architectures would seem to call every one of our claims into question: Is dynamic binding necessary? Large Language Models (LLMs) don\u2019t do dynamic binding, and yet they converse like a person. Are multi-place predicates necessary? LLMs don\u2019t form multi-place predicates, and yet they know more than anyone I\u2019ve ever met. Is structure mapping necessary? LLMs don\u2019t do structure mapping, and yet they succeed on many of the tasks you cite as requiring symbolic thought. Indeed, transformer models seem to obviate all these things \u2014 dynamic binding, multi-place predicates, and even structure mapping and CWSG \u2014 via their attention heads and extensive training.  Webb et al. (2023) showed that some LLMs appear to solve at least some analogies, which would suggest some capacity to at least approximate symbolic thought. These \n\n--- Page 40 ---\nsuccesses cast doubt on the need for explicit structure mapping in analogical reasoning. However, it worth pointing out that it is not well understood exactly how these models solve analogies. It is at least plausible that the immense quantity of text on which LLMs are trained makes it possible for them to solve such problems based only on the statistics of their training corpora. It is also possible that the dynamic weighting mechanisms in a transformer are similar to, or a substitute for, dynamic binding in biological mechanisms8. However, transformers often struggle to correctly bind representational elements into statistically unusual relations, in part because they are dependent on conjunctions of properties in the examples on which they have been trained. As a result, relational roles are not represented independently of their fillers, rendering the system dependent on familiar role-filler combinations. The effect of entanglement on neural networks can be visually observed by asking a transformer model to create an image of an arrangement of objects with statistically uncommon role-filler bindings. Such a request can cause the network to make nonsensical images, where objects that should be separate become physically connected, or the transformer may just depict the objects in a more statistically likely arrangement instead.  Take for example the prompt, \u201cCan you create a photorealistic image in a 1:1 aspect ratio of a new daytime viewpoint of a tent erected on the wing of a plane which is parked on the wing of seagull?\u201d Three state-of-the-art networks at the time of this writing (ChatGPT o4-mini-high, OpenAI, 2025; ChatGPT o3, OpenAI, 2025; Gemini 2.5 Flash, Google, 2025) showed that they struggled to represent seagulls, planes, and tents independently of each other and in the correct relations. As shown in Figure 8, the networks hybridized the airplanes and seagulls, put objects in the wrong relationships to each other or on the wrong parts of objects, removed parts of objects or added disembodied parts, and they struggled to individuate separate tokens of the same kind of object (that is, they confused the wings of the airplane with the wings of a seagull). People, by contrast, do not struggle to imagine what such a scene would look like, although it is interesting to wonder whether the DBO and MO architectures would. \n 8 Transformers dynamically change connection weights between units, which has been likened to attention, but these weight changes are also a function of the training set. For this reason, transformers do not achieve the kind of fully flexible dynamic binding discussed here. \n\n--- Page 41 ---\n  Figure 8 Images produced by three networks in response to the prompt, \u201cCan you create a photorealistic image in a 1:1 aspect ratio of a new daytime viewpoint of a tent erected on the wing of a plane which is parked on the wing of seagull?\u201d First row: ChatGPT o4-mini-high (OpenAI, 2025), OpenAI\u2019s model that specializes in visual reasoning, tends toward hybridizing the plane and the bird. Second row: ChatGPT o3 (OpenAI, 2025), OpenAI\u2019s advanced reasoning model, also tends to hybridize the plane and the bird. Third row: Google Gemini 2.5 Flash (Google, 2025), displays some tendency toward hybridization, but also reverses relations or adds duplicates of parts when generating images of unusual spatial relations. The first image in each row depicts the network\u2019s best response to the prompt.   \n\n\n--- Page 42 ---\nThe complete set of generated images (the first 12 images generated by each network in response to the prompt) is available at https://osf.io/p5zjw. Finding errors like the examples above will always be a moving target, because as soon as the network is trained with an example that matches the prompt, the conjunction of properties will reside within the convex hull of the network\u2019s training set. While the specific example will no longer give rise to an error, the class of errors will persist.  The result of these systems\u2019 reliance on entangled representations is that they require an enormous amount of training to perform any task. Being limited, for example, to representing \u201cthis shape in this location in the visual field, at this size, in this color, in this viewpoint, in this relational role\u201d means that, to recognize a given object in any location, size, color, viewpoint, or relationship to other objects, the networks must effectively be trained on every object in every possible location, size, color, viewpoint, and relationship (see Montero et al., 2020; 2022; Schott et al., 2021) in order to try to expand the convex hull. LLMs as models of human cognition must be trained on a body of text that it would take a human thousands of years to read, even assuming said human did nothing but read the entire time.  A human, by contrast, learns to converse intelligently within about three years. Children learn words in at most two or three examples (Smith & Yu, 2008). People can also recognize a novel object at any location, size, color, viewpoint, and relationship to other any other object after just a single viewing (barring \u201caccidental\u201d views that make it impossible to recover the object\u2019s shape; Biederman, 1987). People extrapolate as a matter of course. Clearly, biological intelligence comes equipped with capacities that do not depend on extensive training and are not limited to the convex hull of the training set. This capacity is true of animals whether or not they have the capacity of for symbolic thought.  One of the advantages of the biological approach appears to be resource efficiency. If one of the goals for artificial intelligence is to create a system that behaves like humans and runs on the power of a single light bulb rather than producing the carbon footprint of New York City operating for a month, it may be useful to look for inspiration from biological intelligence. Our goal in this paper was to explore how biological intelligence achieves symbolic cognition. LISA, while not an image computable model, is an explainable, biologically-inspired neural model of human reasoning. In the simulations reported here, the only prior examples on which LISA was \u201ctrained\u201d were the Memory analogs. In other words, the DBO, MO, RO, and R&M architectures explored here each made the inferences they did based on only a single training example. Principles from the LISA architecture could be useful for developing artificial neural networks that, like people and other animals, are less resource intensive and require less training.  Conclusion  Our simulations results suggest that the difference between most biological intelligence and human intelligence is that we, possibly along a few other notable examples, evolved the ability to integrate multiple dynamic role bindings into multi-place relations along with the ability to map systems of such relations onto one another. Together, these abilities make it possible to (a) represent and reason about the relations among multiple entities (either objects or relations) in the universe, (b) form recursive structures that qualitatively expand the capacity of our mental representations, and (c) map representations onto one another, making it possible for us to use the symbolic representations we can generate.   \n\n--- Page 43 ---\nReferences  Abraham, F. D., Abraham, R. H., & Shaw, C. D. (1990). A visual introduction to dynamical systems theory for psychology. Aerial Press. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., ... & McGrew, B. (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Ardestani, A., Shen, W., Darvas, F., Toga, A. W., & Fuster, J. M. (2016). Modulation of frontoparietal neurovascular dynamics in working memory. Journal of Cognitive Neuroscience, 28(3), 379-401. Ba, J., Mnih, V., & Kavukcuoglu, K. (2014). Multiple object recognition with visual attention. arXiv preprint arXiv:1412.7755. Batheiller, B., Gschwend, O., & Carleton, A. (2010). Temporal coding in olfaction. In A. Menin (Ed.). The Neurobiology of Olfaction. CRC Press/Taylor & Franics, Boca Rarton, FL. Bird, C. D., & Emery, N. J. (2009). Insightful problem solving and creative tool modification by captive nontool-using rooks. Proceedings of the National Academy of Sciences, 106(25), 10370-10375. Biederman, I. (1987).  Recognition-by-components:  A theory of human image understanding.  Psychological Review, 94, 115-147. Blaisdell, A. P., Sawa, K., Leising, K. J., & Waldmann, M. S. (2006). Causal reasoning in rats. Science, 311, 1020-1022. Boesch, C., & Boesch, H. (1990). Tool use and tool making in wild chimpanzees. Folia primatologica, 54(1-2), 86-99. Boole, G. (1847). The Mathematical Analysis of Logic. Queens College, Cork (now University College Cork). Ireland: Monograph. Boole, G. (1854). An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities. Queens College, Cork (now University College Cork). Ireland: Monograph. Castro, A. A., Hummel, J. E., & Berenbaum, H. (2023). An experimental and simulation study of the impact of emotional information on analogical reasoning. Cognition, 238, 105510. https://doi.org/10.1016/j.cognition.2023.105510 Cheng, P. W. (1997). From covariation to causation: A causal power theory. Psychological Review, 104, 367-405.  Chomsky, N. (1972). Language and Mind. New York: Harcourt Brace Jovanovich. Clayton, N., & Emery, N. (2005). Corvid cognition. Current biology, 15(3), R80-R81. Deacon, T. W. (1997). The Symbolic Species: The Co-evolution of Langrage and the Brain. New York: Norton. Doerig, A., Sommers, R. P., Seeliger, K., Richards, B., Ismael, J., Lindsay, G. W., ... & Kietzmann, T. C. (2023). The neuroconnectionist research programme. Nature Reviews Neuroscience, 24(7), 431-450. Doumas. L. A. A., Hummel, J. E., & Sandhofer, C. M.  (2008).  A theory of the discovery and predication of relational concepts.  Psychological Review, 115, 1 - 43. Doumas, L. A. A., Puebla, G., Martin, A. E. & Hummel, J. E. (2022). A theory of relation learning and cross-domain generalization. Psychological Review. https://doi.org/10.1037/rev0000346 Esser, P., Rombach, R., & Ommer, B. (2021). Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 12873-12883). Everett, D. L. (1986). Pirah\u00e3. Handbook of Amazonian languages. Berlin: Mouton de Gruyter. \n\n--- Page 44 ---\nFaulkenhainer, B., Forbus, K. D., & Gentner, D. (1989).  The structure mapping engine:  Algorithm and examples.  Artificial Intelligence, 41, 1-63. Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2), 3-71. Fuster, J. M. (2009). Cortex and memory: emergence of a new paradigm. Journal of cognitive neuroscience, 21(11), 2047-2072. Galizia C. G., & Sachse S. (2010). Odor coding in insects. In: Menini A, Ed. The Neurobiology of Olfaction. Boca Raton (FL): CRC Press/Taylor & Francis; 2010. Garcia, J., W. G. Hankins & K. W. Rusiniak. (1974). Behavioral regulation of the milieu interne in man and rat. Science, 184:  824\u2013831. Gentner, D. (1983). Structure-mapping: A theoretical framework for analogy. Cognitive Science, 7, 155-170.  Gibson, J. J. (1977). The theory of affordances. Perceiving, acting and knowing: Towards an ecological psychology. Erlbaum. Gick, M. L., & Holyoak, K. J. (1980). Analogical problem solving. Cognitive Psychology, 12, 306-355. Gick, M. L., & Holyoak, K. J. (1983). Schema induction and analogical transfer. Cognitive Psychology, 15, 1-38. Giri, T., & Garcia\u2010Pelegrin, E. (2025). Opportunistic Tool Use by Two Unexpected Corvid Species. Ecology and Evolution, 15(5), e71314. Gray, C. M. & Singer, W. (1989). Stimulus-specific neuronal oscillations in orientation columns of cat visual cortex. Proc. Natl. Acad. Sci., 86, 1698-1702.  Goodale, M. A., & Milner, A. D. (1992). Separate visual pathways for perception and action. Trends in Neuroscience, 15(1), 20-15.  Halford, G. S., Bain, J. D., Maybery, M. T., & Andrews, G. (1998). Induction of relational schemas: Common processes in reasoning and complex learning. Cognitive Psychology, 35 (3): 201-245. Halford, G. S., & Busby, J. (2007). Acquisition of structured knowledge without instruction: the relational schema induction paradigm. Journal of Experimental Psychology: Learning, Memory and Cognition, 33 (3): 586-603. He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). Heaton, R. F., & Hummel, J. E. (2019). Rapid unsupervised encoding of object files for visual reasoning. Proceedings of the 41st Annual Meeting of the Cognitive Science Society, 1895-1902. Holyoak, K. J., & Thagard, P. (1989). Analogical mapping by constraint satisfaction. Cognitive Science, 13, 295-355. Holyoak, K. J., & Thagard, P. (1995).  Mental leaps:  Analogy in creative thought.  Cambridge, MA:  MIT Press. Hummel, J. E. (2011). Getting symbols out of a neural architecture. Connection Science, 23, 109-118. Hummel, J. E., & Biederman, I.  (1992). Dynamic binding in a neural network for shape recognition. Psychological Review, 99, 480-517. Hummel, J. E., & Heaton, R. F. (2025, August 19). Code and Simulations for A Computational Phylogenesis of Symbolic Thought. https://doi.org/10.17605/OSF.IO/P5ZJW Hummel, J. E., & Holyoak, K. J. (1997).  Distributed representations of structure: A theory of analogical access and mapping.  Psychological Review, 104, 427-466. Hummel, J. E., & Holyoak, K. J.  (2003). A symbolic-connectionist theory of relational inference and generalization.  Psychological Review, 110, 220-264. \n\n--- Page 45 ---\nHummel, J. E., & Landy, D. H. (2009).  From analogy to explanation: Relaxing the 1:1 mapping constraint\u2026 Very carefully. In B. Kokinov, K. Holyoak & D. Gentner (Eds.) New Frontiers in Analogy Research: Proceedings of the Second International Conference on Analogy. Sofia, Bulgaria. Hummel, J. E., Landy, D. H., & Devnich, D.  (2008).  Toward a process model of explanation with implications for the type-token problem.  In Naturally Inspired AI: Papers from the AAAI Fall Symposium.  Technical Report FS-08-06, 79-86. Hummel, J. E., Licato, J., & Bringsjord, S. (2014). Analogy, explanation, and proof. Frontiers in Human Neuroscience. http://journal.frontiersin.org/Journal/10.3389/fnhum.2014.00867/abstract Kellman, P. J., Burke, T. & Hummel, J. E.  (1999).  Modeling perceptual learning of abstract invariants.  Proceedings of the 21st Annual Conference of the Cognitive Science Society (pp. 264-269).   Mahwah, NJ:  Erlbaum. Kahneman, D., Treisman, A., & Gibbs, B. J. (1992). The reviewing of object files: Object-specific integration of information. Cognitive psychology, 24(2), 175-219. Kim, J. G., & Biederman, I. (2011). Where do objects become scenes? Cerebral Cortex, 21(8), 1738-1746. Knowlton, B. J., Morrison, R. G., Hummel, J. E., & Holyoak, K. J. (2012). A neurocomputational system for relational reasoning. Trends in Cognitive Sciences, 17, 373-381. Kubose, T. T., Holyoak, K. J., & Hummel, J. E. (2002). The role of textual coherence in incremental analogical mapping. Journal of Memory and Language, 47(3), 407\u2013435. https://doi.org/10.1016/S0749-596X(02)00011-6 Lake, B., & Baroni, M. (2018, July). Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning (pp. 2873-2882). PMLR. Landy, D. H. & Hummel, J. E. (2010). Explanatory reasoning for inductive confidence. In Proceedings of the 32nd Annual Conference of the Cognitive Science Society. Licato, J., Bringsjord, S., and Hummel, J. E. (2012). Exploring the role of analogico-deductive reasoning in the balance-beam task. In Rethinking Cognitive Development : Proceedings of the 42nd Annual Meeting of the Jean Piaget Society. Loukola, O. J., Solvi, C., Coscos, L., & Chittka, L. (2017). Bumblebees show cognitive flexibility by improving on an observed complex behavior. Science, 355(6327), 833-836. Malhotra, G., Evans, B. D., & Bowers, J. S. (2020). Hiding a plane with a pixel: examining shape-bias in CNNs and the benefit of building in biological constraints. Vision Research, 174, 57-68. Marcus, G. F. (1998). Rethinking eliminative connectionism. Cognitive psychology, 37(3), 243-282. Martin-Ordas, G. (2022). Spontaneous relational and object similarity in wild bumblebees. Biology letters, 18(8), 20220253. Martin-Ordas, G. (2023). Relational reasoning in wild bumblebees revisited: the role of distance. Scientific Reports, 13(1), 22311. Montero, M. L., Ludwig, C. J. H., Costa, R. P., Malhotra, G., & Bowers, J. (2020). The role of disentanglement in generalization. In International Conference on Learning Representations, 2021.  Montero, M. L., Bowers, J., Costa, R. P., Ludwig, C. J. H., Malhotra, G. (2022), Lost in latent space: Examining failures of disentangled models at combinatorial generalization. Advances in Neural Information Processing Systems, 35, 10136-10149.  Moreno, A. M., de Souza, D. D. G., & Reinhard, J. (2012). A comparative study of relational learning capacity in honeybees (Apis mellifera) and stingless bees (Melipona rufiventris). PLoS One, 7(12), e51467.  \n\n--- Page 46 ---\nNg, L., Garcia, J. E., & Dyer, A. G. (2020). Use of temporal and colour cueing in a symbolic delayed matching task by honey bees. Journal of Experimental Biology, 223(15), jeb224220. OpenAI. (2022). DALL-E 2 (Oct 24 version) [Text to Image Large Language Model]. https://openai.com/index/dall-e-2. Peirce, C. S. (1897, 1903). Logic as semiotic: The theory of signs. In J. Buchler (Ed.), The Philosophical Writings of Peirce (1955). New York: Dover Books, 98-119. Peirce, C. S. (1934).  Collected papers, Vol. V, Pragmatism and pragmaticism (C. Hartshorne & P. Weiss, Eds.).  Cambridge, MA:  Harvard University Press. Penn, D., Holyoak, K., & Povinelli, D. (2008). Darwin's mistake: Explaining the discontinuity between human and nonhuman minds. Behavioral and Brain Sciences, 31(2), 109-130. doi:10.1017/S0140525X08003543 Pitman, R. L., & Durban, J. W. (2012). Cooperative hunting behavior, prey selectivity and prey handling by pack ice killer whales (Orcinus orca), type B, in Antarctic Peninsula waters. Marine Mammal Science, 28(1), 16-36. Povinelli, D. (2003). Folk physics for apes: The chimpanzee's theory of how the world works. Oxford University Press. Quilty-Dunn, J., Porot, N., and Mandelbaum, E. (2023). The best game in town: the re-emergence of the Language of Thought hypothesis across the cognitive sciences. Behavioral and Brain Sciences. 46:e261. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). Ross, B. (1989).  Distinguishing types of superficial similarities: Different effects on the access and use of earlier problems. Journal of Experimental Psychology: Learning, Memory, and Cognition, 15, 456-468. Rumelhart, D. E., McClelland, J. L., & the PDP Research Group (1986). Parallel distributed processing: Explorations in the microstructure of cognition, Vol. 1 (pp. 318-362). Cambridge, MA: MIT Press. Schott, L., von K\u00fcgelgen, J., Tr\u00e4uble, F., Gehler, P., Russell, C., Bethge, M., Sch\u00f6lkopf, B., Sch\u00f6lkopf, F., & Brendel, W. (2021). Visual representation learning does not generalize strongly within the same domain. arXiv preprint arXiv:2107.08221, 2021. Searle, J. (1980). \u201cMinds, Brains, and Programs.\u201d Behavioral and Brain Sciences 3, 417-424. Shastri, L., & Ajjanagadde, V. (1993). From simple associations to systematic reasoning: A connectionist representation of rules, variables and dynamic bindings using temporal synchrony. Behavioral and Brain Sciences, 16(3), 417-494.  https://doi.org/10.1017/S0140525X00030910 Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556. Singer, W. (1999). Neuronal synchrony: a versatile code for the definition of relations?. Neuron, 24(1), 49-65. Singer, W. (2007). Binding by synchrony. Scholarpedia, 2(12), 1657. Skinner, B. F. (1957). Verbal behavior. Appleton-Century-Crofts. https://doi.org/10.1037/11256-000  Smith, L. & Yu, C. (2008). Infants rapidly learn word-referent mappings via cross-situational statistics. Cognition, 106, 1558-1568. Stopfer, M. (2007). Olfactory processing: massive convergence onto sparse codes. Current biology, 17(10), R363-R364.  \n\n--- Page 47 ---\nStrong, G. W., & Whitehead, B. A. (1989). A solution to the tag-assignment problem for neural networks. Behavioral and Brain Sciences, 12(3), 381-397. Tallon-Baudry, C., Mandon, S., Freiwald, W. A., & Kreiter, A. K. (2004). Oscillatory synchrony in the monkey temporal lobe correlates with performance in a visual short-term memory task. Cerebral cortex, 14(7), 713-720. Tanaka, N. K., Ito, K., & Stopfer, M. (2009). Odor-evoked neural oscillations in Drosophila are mediated by widely branching interneurons. Journal of Neuroscience, 1 (26): 8595-8603. https://doi.org/10.1523/JNEUROSCI.1455-09.2009 Thompson, R. K. R., Oden, D. L., & Boysen, S. T. (1997). Language-naive chimpanzees (Pan troglodytes) judge relations between relations in a conceptual matching-to-sample task. Journal of Experimental Psychology: Animal Behavior Processes, 23(1), 31-43. https://doi.org/10.1037/0097-7403.23.1.31 Tomasello, M., & Call, J. (1997). Primate cognition. Oxford University Press, USA. Turner, G. C., Bazhenov M., & Laurent G. (2007). Olfactory representations by Drosophila mushroom body neurons. Journal of Neurophysiology, 99:734\u201346. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., Polosukhin, I. (2017). Attention is all you need. arXiv: 1706.03762. https://doi.org/10.48550/arXiv.1706.03762 Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3156-3164). von der Malsburg, C., & Buhmann, J. (1992). Sensory segmentation with coupled neural oscillators. Biological Cybernetics, 67, 233-242.   Webb, T., Holyoak, K. J., & Lu, H. (2023). Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9), 1526-1541. Wilner, S., & Hummel, J. E., (2017). Automatic encoding of the LISA model of analogy from raw text. Proceedings of the 28th Modern Artificial Intelligence and Cognitive Science, 115-122. http://ceur-ws.org/Vol-1964/NLP3.pdf Xu, K. (2015). Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044.\n",
  "project_dir": "artifacts/projects/enhanced_cs.NE_2508.15082v1_From_Basic_Affordances_to_Symbolic_Thought_A_Comp",
  "communication_dir": "artifacts/projects/enhanced_cs.NE_2508.15082v1_From_Basic_Affordances_to_Symbolic_Thought_A_Comp/.agent_comm",
  "assigned_at": "2025-08-22T21:05:27.726871",
  "status": "assigned"
}